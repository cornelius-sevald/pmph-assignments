PMPH Assignment 1
===============

By: Cornelius Sevald-Krause `<lgx292>`  
Due: 2022-09-15

Task 1
------

### Part a)

Let `a, b, c` be elements of `Img(h)` such that `h x = a`, `h y = b` and
`h z = c` where `x, y, z` are elements of the domain of `h`.

To prove that '`o`' is associative we write the expression `(a o b) o c` as
`(h x o h y) o h z`. Using the third definition of `h` we re-write it
`h (x ++ y) o h z = h (x ++ y) ++ z`.
As list concatenation is associative, we can further re-write:
```
h (x ++ y) ++ z     =
h x ++ (y ++ z)     =
h x o h y ++ z      =
h x o (h y o h z)   =
a o (b o c)
```

To prove that `e` is the neutral element we use the first and third definitions
of `h` to write `b o e` as `h y o h [] = h y ++ [] = h y = b`.
It is also easy to see that `b o e = h y ++ [] = h [] ++ y = e o b`.

### Part b)

As `reduce (++) [] . distr_p` is the identity function we can write
`reduce (+) 0 . map f` as `reduce (+) 0 . map f . reduce (++) [] . distr_p`.
Using the second, third and first lemma (in that order) we get:
```
reduce (+) 0 . reduce (++) [] . map (map f) . distr_p       =
reduce (+) 0 . map (reduce (+) 0) . map (map f) . distr_p   =
reduce (+) 0 . map ( (reduce (+) 0) . (map f) ) . distr_p   =
```

Task 2
------

### Solution

The solution added to `lssp.fut` is shown below (line 21-25):
```futhark
let connect= tlx == 0 || tly == 0 || pred2 lastx firsty
let newlss = max (max (lssx, lssy), if connect then (lcsx + lisy) else 0)
let newlis = if lisx == tlx && connect then lisx + lisy else lisx
let newlcs = if lcsy == tly && connect then lcsx + lcsy else lcsy
let newtl  = tlx + tly
```

### Test cases

The following test cases were added.  
for `lssp-same.fut`:
```
compiled input {
   [0i32, 1, 2, 3, 4, 5, 6, 7, 8, 9]
}
output {
   1
}
compiled input {
   [1i32, 2, 2, 4, 4, 4, 4, 3, 3, 3]
}
output {
   4
}
```

for `lssp-zeros.fut`:
```
compiled input {
   [0i32, 1, 2, 3, 4, 5, 6, 7, 8, 9]
}
output {
   1
}
compiled input {
   [1i32, 2, 2, 4, 4, 4, 4, 3, 3, 3]
}
output {
   0
}
```

for `lssp-sorted.fut`:
```
compiled input {
   [0i32, 1, 2, 3, 4, 5, 6, 7, 8, 9]
}
output {
   10
}
compiled input {
   [9i32, 8, 7, 6, 5, 4, 3, 2, 1, 0]
}
output {
   1
}
```

### Speedup

The benchmarks are generated by running
`futhark dataset --i32-bounds=-10:10 -b -g [10000000]i32`
and piping it into `./$prog -t /dev/stderr -r 10`
where `$prog` is one of `lssp-same`, `lssp-sorted` and `lssp-zeros`.
The average is then taken of the ten reported values.

The reported speed of `lssp-same`, `lssp-sorted` and `lssp-zeros` respectively,
**without** acceleration is:
```
24445
24579
18240
```

The reported speed **with** acceleration is:
```
2943
2966
2946
```

giving a speedup of approx. `8.31`, `8.29` and `6.19` respectively.

Task 3
------

The CUDA program validates and an epsilon of `1e-5`.
The sweet-point seems to be around `N=400` (on gpu02).
A table of input sizes and speedups are given below.

| N    | SPDUP |
|------|-------|
| 2^9  |     1 |
| 2^10 |     2 |
| 2^11 |     7 |
| 2^12 |    11 |
| 2^13 |    22 |
| 2^14 |    43 |
| 2^15 |    62 |
| 2^16 |    77 |
| 2^17 |    98 |
| 2^18 |   104 |
| 2^19 |   110 |
| 2^20 |   117 |
| 2^21 |   119 |
| 2^22 |   117 |
| 2^23 |   122 |

There are drastic increases in speedups between N=2^13 and N=2^17.
The maximum speedup seems to be a little above 120 which is reached at N=2^23.

The code of the CUDA kernel is given below:
```c
__global__ void parallel_map(float *d_in, float *d_out, unsigned int N) {
    const unsigned int lid = threadIdx.x;
    const unsigned int gid = blockIdx.x*blockDim.x + lid;
    if (gid < N) {
        float x = d_in[gid];
        float y = (x/(x-2.3))*(x/(x-2.3))*(x/(x-2.3)); // (x/(x-2.3))^3
        d_out[gid] = y;
    }
}
```
The local thread ID is fetched and used with the block ID to calculate the
global ID. If the global ID exceeds the input array size the thread is skipped.
Otherwise it preforms the calculation and stores it in the output
according to its global ID.

The code calling the kernel is given below:
```c
// copy host memory to device
cudaMemcpy(d_in, h_in, mem_size, cudaMemcpyHostToDevice);

// preform parallel map
gettimeofday(&t_start, NULL);
for (int i = 0; i < GPU_RUNS; i++) {
    parallel_map<<<num_blocks, block_size>>>(d_in, d_out, N);
} cudaThreadSynchronize();
gettimeofday(&t_end, NULL);
timeval_subtract(&t_diff, &t_end, &t_start);
gpu_elapsed = (t_diff.tv_sec*1e6 + t_diff.tv_usec) / GPU_RUNS;

// copy host memory to device
cudaMemcpy(h_out_p, d_out, mem_size, cudaMemcpyDeviceToHost);
```
First the input is copied to the device, then the start time is recorded
before calling the kerne `GPU_RUNS` times.
After the loop the threads are synchronized and the end time is recorded.
From the start and end time the elapsed time is calculated and the result
is copied from the device memory to the host.

The code that computes the grid and block sizes is given below:
```c
unsigned int block_size = 256;
unsigned int num_blocks = ((N + (block_size - 1)) / block_size);
```

The block size is chosen to be 256 as that's what was used in an example in
the CUDA documentation. From that, the number of blocks is calculated.
As integer division rounds down we need to add `(block_size - 1)` to `N` before
dividing to make sure that there are enough blocks in cases where `block_size`
does not divide `N` evenly.

Task 4
======

### Implementation

The flat-parallel implemention is given below:
```futhark
let spMatVctMult [num_elms] [vct_len] [num_rows] 
                 (mat_val : [num_elms](i64,f32))
                 (mat_shp : [num_rows]i64)
                 (vct : [vct_len]f32) : [num_rows]f32 =

  let prds = map (\(i,x) -> x*vct[i]) mat_val
  let flags  = mkFlagArray mat_shp mat_val
  let indsp1 = scan (+) 0 mat_shp
  let sc_arr = sgmSumF32 flags prds
  in  map (\ip1 -> sc_arr[ip1-1]) indsp1
```

Line 6 (`let prds = ...`) uses the indices of `mat_val` to multiply the elements
of the vector with the elements of the matrix.

Line 7 uses the previously defined `mkFlagArray` function to create an array
of booleans that can be used in a segmented scan.
The `mkFlagArray` implementation is a slightly modified version of the function
from p. 48 of the lecture notes.

Line 8 uses a scan to get a list of indices. These indices are all 1 greater
than they should be, hence the name "indices plus 1" -> `indsp1`.

Line 9 uses the `sgmSumF32` function to preform a segmented sum on the products
from line 6 and using the flag array from line 7. The final value of each
segment is the value that is actually needed.

Line 10 extracts those values from `sc_arr` using the indices plus one array.

### Speedup

To get the runtime of the sequential and flat-parallel programs the command
`furhark dataset --i64-bounds=0:9999 -g [1000000]i64 --f32-bounds=-7.0:7.0 -g [1000000]f32 --i64-bounds=100:100 -g [10000]i64 --f32-bounds=-10.0:10.0 -g [10000]f32`
was used to generate the data and then piped to the program with the `-r 10`
flag to run it 10 times.

The sequential version using the C backend had an average runtime of 1625 ms.
The flat-parallel version using the CUDA backend had an average runtime of
363 ms resulting in a speedup of ~4.48.
